{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!python\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Dense, Activation, LSTM, Bidirectional\n",
    "from tensorflow.python.keras import metrics\n",
    "from helper_funct import *\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TODO: gradient back propagation\n",
    "#       freeze weights and load them for more layers. Train stack\n",
    "#       use 100 units to find best method.\n",
    "#       Camelyon17 images. Use CNN network\n",
    "#       combine image and MSI data. Image captioning\n",
    "#       stateful = true, or return sequences in last layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HYPER Parameters\n",
    "mini_batch_size = 32\n",
    "embedding_size = 8\n",
    "learning_rate = 0.001 # 0.005\n",
    "epoch_train = 300  # maximum repetitions\n",
    "validation_split = 0.05\n",
    "optimizer_str = \"RMSprop\"\n",
    "optimizer = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0) # keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True) #\n",
    "metrics = ['accuracy', 'mae']\n",
    "bias_init = 'he_normal'  # It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where  fan_in is the number of input units in the weight tensor.\n",
    "kernel_init = 'he_normal'\n",
    "weight_init = 'he_normal'\n",
    "use_bias = True\n",
    "verbose = 2\n",
    "shuffle = True\n",
    "\n",
    "# Model parameters\n",
    "units = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "layers = [1, 2, 3, 5, 8, 10]\n",
    "lstm_type = ['LSTM', 'Bidirectional']\n",
    "activation = 'softmax'\n",
    "loss_function = 'binary_crossentropy'\n",
    "merge_mode = ['ave', 'concat', 'sum']\n",
    "\n",
    "units = [50]\n",
    "layers = [2]\n",
    "lstm_type = ['LSTM']\n",
    "merge_mode = 'sum'\n",
    "\n",
    "file = open(\"results.txt\", \"a\")\n",
    "file.write(\"mini_batch_size: {} learning_rate: {} optimizer: {}\".format(mini_batch_size, learning_rate, optimizer_str))\n",
    "\n",
    "\n",
    "# Data specific parameters\n",
    "n_sets = 8\n",
    "n_folds = 4\n",
    "num_classes = 2\n",
    "time_steps = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialing the output array\n",
    "balanced_accuracy = np.zeros((len(units), len(layers), len(lstm_type)))\n",
    "run_time2 = np.zeros((len(units), len(layers), len(lstm_type)))\n",
    "\n",
    "# Reading the data from the read_data function\n",
    "x_train, x_test, x_val, y_val, y_train, y_test = read_data(n_folds)\n",
    "\n",
    "# in_shape defines the input shape of the LSTM modules\n",
    "in_shape = len(x_train[0][0][0])  # data length variable for the input tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit:  50\nn_layers:  2\ntype:  LSTM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tensorflow.python.keras._impl.keras.layers.recurrent.LSTM object at 0x7ff6c3b257d0>, False)\nLSTM_1\n(<tensorflow.python.keras._impl.keras.layers.recurrent.LSTM object at 0x7ff6c2e3aa50>, True)\nLSTM_2\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nLSTM_1 (LSTM)                (32, None, 50)            944400    \n_________________________________________________________________\nLSTM_2 (LSTM)                (32, 50)                  20200     \n_________________________________________________________________\ndense_1 (Dense)              (32, 2)                   102       \n_________________________________________________________________\nactivation_1 (Activation)    (32, 2)                   0         \n=================================================================\nTotal params: 964,702\nTrainable params: 20,302\nNon-trainable params: 944,400\n_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloning weights: models_saved/Weights_layers:1_Type:LSTM_units:50_Set:0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloning weights: models_saved/Weights_layers:1_Type:LSTM_units:50_Set:1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloning weights: models_saved/Weights_layers:1_Type:LSTM_units:50_Set:2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloning weights: models_saved/Weights_layers:1_Type:LSTM_units:50_Set:3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1.4205 - acc: 0.5347 - mean_absolute_error: 0.4704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 1.0280 - acc: 0.5384 - mean_absolute_error: 0.4659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for unit in units:\n",
    "    for n_layers in layers:\n",
    "        for type in lstm_type:\n",
    "            print \"unit: \", unit\n",
    "            print \"n_layers: \", n_layers\n",
    "            print \"type: \", type\n",
    "            start_time2 = time.time()\n",
    "            model = Sequential()\n",
    "            # Adding the LSTM layers or the Bidirectional LSTM modules\n",
    "            if type == 'LSTM':\n",
    "                if n_layers > 1:\n",
    "                    model.add(LSTM(units=unit, batch_input_shape=(mini_batch_size, time_steps, in_shape), return_sequences=True, stateful=True,\n",
    "                                   bias_initializer=bias_init, kernel_initializer=kernel_init,\n",
    "                                   recurrent_initializer=weight_init, use_bias=use_bias, name=\"LSTM_1\"))\n",
    "                    for j in range(2, n_layers):\n",
    "                        model.add(LSTM(units=unit, return_sequences=True, stateful=True, bias_initializer=bias_init,\n",
    "                                       kernel_initializer=kernel_init, recurrent_initializer=weight_init,\n",
    "                                       use_bias=use_bias, name = \"LSTM_{}\".format(j)))\n",
    "                    model.add(LSTM(units=unit, bias_initializer=bias_init, kernel_initializer=kernel_init,\n",
    "                                   recurrent_initializer=weight_init, use_bias=use_bias, name=\"LSTM_{}\".format(n_layers)))\n",
    "                else:\n",
    "                    model.add(LSTM(units=unit, input_shape=(time_steps, in_shape), bias_initializer=bias_init,\n",
    "                                   kernel_initializer=kernel_init, recurrent_initializer=weight_init,\n",
    "                                   use_bias=use_bias, name=\"LSTM_1\"))\n",
    "            else:\n",
    "                if n_layers > 1:\n",
    "                    model.add(Bidirectional(LSTM(units=unit, return_sequences=True, stateful=True, bias_initializer=bias_init, kernel_initializer=kernel_init, \n",
    "                                                 recurrent_initializer=weight_init, use_Bias=use_bias), batch_input_shape=(mini_batch_size, time_steps, in_shape), \n",
    "                                            merge_mode=merge_mode))\n",
    "                    for j in range(1, n_layers-1):\n",
    "                        model.add(Bidirectional(LSTM(units=unit, return_sequences=True, stateful=True, bias_initializer=bias_init, kernel_initializer=kernel_init, \n",
    "                                                     recurrent_initializer=weight_init, use_Bias=use_bias), merge_mode=merge_mode))\n",
    "                    model.add(Bidirectional(LSTM(units=unit, bias_initializer=bias_init, kernel_initializer=kernel_init, recurrent_initializer=weight_init, \n",
    "                                                 use_Bias=use_bias), merge_mode=merge_mode))\n",
    "                else:\n",
    "                    model.add(Bidirectional(LSTM(units=unit, bias_initializer=bias_init, kernel_initializer=kernel_init, recurrent_initializer=weight_init \n",
    "                                                 ), input_shape=(time_steps, in_shape), merge_mode=merge_mode))\n",
    "\n",
    "            models_recovering = [Sequential() for _ in xrange(n_folds)]\n",
    "            i = n_layers - 1\n",
    "            layers_to_load = 0\n",
    "            isloaded = False\n",
    "            while i > 0 and not isloaded:\n",
    "                file_name = \"models_saved/Weights_layers:{}_Type:{}_units:{}_Set:0.h5\".format(i, type, unit)\n",
    "                if os.path.isfile(file_name):\n",
    "                    isloaded = True\n",
    "                    layers_to_load = i\n",
    "                i -= 1\n",
    "            if isloaded:\n",
    "                for layer in model.layers[:i+1]:\n",
    "                    layer.trainable = False\n",
    "\n",
    "            for layer in model.layers:\n",
    "                print(layer, layer.trainable)\n",
    "                print(layer.name)\n",
    "\n",
    "            # Adding the rest of the network's components\n",
    "            model.add(Dense(units=num_classes))\n",
    "            model.add(Activation(activation=activation))\n",
    "            # model.compile(loss=loss_function, optimizer=optimizer, metrics=metrics)\n",
    "            model.summary()\n",
    "\n",
    "            # training of the model\n",
    "            bal_accuracy = train_model(x_train, y_train, x_val, y_val, validation_split, epoch_train, mini_batch_size, shuffle,\n",
    "                                            x_test, y_test, model, n_folds, learning_rate, optimizer, verbose,\n",
    "                                            loss_function, metrics, isloaded, n_layers, layers_to_load, type, unit)\n",
    "\n",
    "            # Saving the balanced accuracy over the 4 folds\n",
    "            balanced_accuracy[units.index(unit), layers.index(n_layers), lstm_type.index(type)] = bal_accuracy\n",
    "            run_time2[units.index(unit), layers.index(n_layers), lstm_type.index(type)] = time.time() - start_time2\n",
    "            print \"run time of units {} n_layers {} of type {} : {}\\n\\n---------------------\\n\\n\".format(unit, n_layers, type, run_time2[units.index(unit), layers.index(n_layers), lstm_type.index(type)])\n",
    "\n",
    "            file.write(\" units: {} n_layers: {} type: {} balanced accuracy: {}\\n\".format(unit, n_layers, type, bal_accuracy))\n",
    "\n",
    "            del model\n",
    "\n",
    "run_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nbalanced_accuracy: \n[[[0.7320036]]]\nrun_time2: \n[[[574.3476429]]]\nTotal run time:  574.348674059\n\nSuccessfully trained and run with balanced_accuracy_final:  0.7320035992049041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"\"\n",
    "print \"balanced_accuracy: \"\n",
    "print balanced_accuracy\n",
    "print \"run_time2: \"\n",
    "print run_time2\n",
    "# print \"\"\n",
    "\n",
    "print \"Total run time: \", run_time\n",
    "\n",
    "balanced_accuracy_final = balanced_accuracy.mean()\n",
    "\n",
    "print(\"\")\n",
    "print \"Successfully trained and run with balanced_accuracy_final: \", balanced_accuracy_final\n",
    "\n",
    "file.write(\"time: {}\\n\\n\".format(run_time))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
